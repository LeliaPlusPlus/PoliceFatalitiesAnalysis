{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Downloading https://files.pythonhosted.org/packages/d2/b1/105fe9a289e5bb64ec104076546f72060296d9989a0fc31a8b608c810868/Scrapy-2.2.0-py2.py3-none-any.whl (241kB)\n",
      "Collecting zope.interface>=4.1.3 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/2c/3e/a2360221be5d3d6f272c7b2a036942cf16c24483d249c141b18297f17a86/zope.interface-5.1.0-cp36-cp36m-win_amd64.whl (194kB)\n",
      "Collecting Twisted>=17.9.0 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/34/78c513dee7ffa0f897fda8623f571e2b6aca0f504cdb91da65e2ac492074/Twisted-20.3.0-cp36-cp36m-win_amd64.whl (3.1MB)\n",
      "Collecting parsel>=1.5.0 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/1e/9b39d64cbab79d4362cdd7be7f5e9623d45c4a53b3f7522cd8210df52d8e/parsel-1.6.0-py2.py3-none-any.whl\n",
      "Collecting service-identity>=16.0.0 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618d43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cryptography>=2.0 in c:\\users\\ch1n-\\anaconda3\\lib\\site-packages (from scrapy) (2.2.2)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in c:\\users\\ch1n-\\anaconda3\\lib\\site-packages (from scrapy) (18.0.0)\n",
      "Collecting itemadapter>=0.1.0 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/fb/92f848fcfa85dc9f95370eaecb5c99b5230dd4fc5c6bae684f4ca59df973/itemadapter-0.1.0-py3-none-any.whl\n",
      "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n",
      "Collecting protego>=0.1.15 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/db/6e/bf6d5e4d7cf233b785719aaec2c38f027b9c2ed980a0015ec1a1cced4893/Protego-0.1.16.tar.gz (3.2MB)\n",
      "Collecting queuelib>=1.4.2 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: lxml>=3.5.0 in c:\\users\\ch1n-\\anaconda3\\lib\\site-packages (from scrapy) (4.2.1)\n",
      "Collecting w3lib>=1.17.0 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/59/b6b14521090e7f42669cafdb84b0ab89301a42f1f1a82fcf5856661ea3a7/w3lib-1.22.0-py2.py3-none-any.whl\n",
      "Collecting cssselect>=0.9.1 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in c:\\users\\ch1n-\\anaconda3\\lib\\site-packages (from zope.interface>=4.1.3->scrapy) (39.1.0)\n",
      "Collecting PyHamcrest!=1.10.0,>=1.9.0 (from Twisted>=17.9.0->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/40/16/e54cc65891f01cb62893540f44ffd3e8dab0a22443e1b438f1a9f5574bee/PyHamcrest-2.0.2-py3-none-any.whl (52kB)\n",
      "Collecting attrs>=19.2.0 (from Twisted>=17.9.0->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/db/4313ab3be961f7a763066401fb77f7748373b6094076ae2bda2806988af6/attrs-19.3.0-py2.py3-none-any.whl\n",
      "Collecting incremental>=16.10.1 (from Twisted>=17.9.0->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n",
      "Collecting Automat>=0.3.0 (from Twisted>=17.9.0->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/dd/83/5f6f3c1a562674d65efc320257bdc0873ec53147835aeef7762fe7585273/Automat-20.2.0-py2.py3-none-any.whl\n",
      "Collecting hyperlink>=17.1.1 (from Twisted>=17.9.0->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/7f/91/e916ca10a2de1cb7101a9b24da546fb90ee14629e23160086cf3361c4fb8/hyperlink-19.0.0-py2.py3-none-any.whl\n",
      "Collecting constantly>=15.1 (from Twisted>=17.9.0->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.6.0 in c:\\users\\ch1n-\\anaconda3\\lib\\site-packages (from parsel>=1.5.0->scrapy) (1.11.0)\n",
      "Collecting pyasn1 (from service-identity>=16.0.0->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
      "Collecting pyasn1-modules (from service-identity>=16.0.0->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\n",
      "Requirement already satisfied: idna>=2.1 in c:\\users\\ch1n-\\anaconda3\\lib\\site-packages (from cryptography>=2.0->scrapy) (2.6)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in c:\\users\\ch1n-\\anaconda3\\lib\\site-packages (from cryptography>=2.0->scrapy) (0.24.0)\n",
      "Requirement already satisfied: cffi>=1.7 in c:\\users\\ch1n-\\anaconda3\\lib\\site-packages (from cryptography>=2.0->scrapy) (1.11.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ch1n-\\anaconda3\\lib\\site-packages (from cffi>=1.7->cryptography>=2.0->scrapy) (2.18)\n",
      "Building wheels for collected packages: PyDispatcher, protego\n",
      "  Running setup.py bdist_wheel for PyDispatcher: started\n",
      "  Running setup.py bdist_wheel for PyDispatcher: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Ch1n-\\AppData\\Local\\pip\\Cache\\wheels\\88\\99\\96\\cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n",
      "  Running setup.py bdist_wheel for protego: started\n",
      "  Running setup.py bdist_wheel for protego: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Ch1n-\\AppData\\Local\\pip\\Cache\\wheels\\51\\01\\d1\\4a2286a976dccd025ba679acacfe37320540df0f2283ecab12\n",
      "Successfully built PyDispatcher protego\n",
      "Installing collected packages: zope.interface, PyHamcrest, attrs, incremental, Automat, hyperlink, constantly, Twisted, cssselect, w3lib, parsel, pyasn1, pyasn1-modules, service-identity, itemadapter, PyDispatcher, protego, queuelib, scrapy\n",
      "  Found existing installation: attrs 18.1.0\n",
      "    Uninstalling attrs-18.1.0:\n",
      "      Successfully uninstalled attrs-18.1.0\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 PyHamcrest-2.0.2 Twisted-20.3.0 attrs-19.3.0 constantly-15.1.0 cssselect-1.1.0 hyperlink-19.0.0 incremental-17.5.0 itemadapter-0.1.0 parsel-1.6.0 protego-0.1.16 pyasn1-0.4.8 pyasn1-modules-0.2.8 queuelib-1.5.0 scrapy-2.2.0 service-identity-18.1.0 w3lib-1.22.0 zope.interface-5.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading https://files.pythonhosted.org/packages/f3/76/4697ce203a3d42b2ead61127b35e5fcc26bba9a35c03b32a2bd342a4c869/tqdm-4.46.1-py2.py3-none-any.whl (63kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.46.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import urljoin, urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "Downloading web-scraping\\python_logo.svg: 5.00B [00:00, 1.25kB/s]\n"
     ]
    }
   ],
   "source": [
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    Checks whether `url` is a valid URL.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "def get_all_images(url):\n",
    "    \"\"\"\n",
    "    Returns all image URLs on a single `url`\n",
    "    \"\"\"\n",
    "    soup = bs(requests.get(url).content, \"html.parser\")\n",
    "    urls = []\n",
    "    for img in tqdm(soup.find_all(\"img\"), \"Extracting images\"):\n",
    "        img_url = img.attrs.get(\"src\")\n",
    "        if not img_url:\n",
    "            # if img does not contain src attribute, just skip\n",
    "            continue\n",
    "        # make the URL absolute by joining domain with the URL that is just extracted\n",
    "        img_url = urljoin(url, img_url)\n",
    "        try:\n",
    "            pos = img_url.index(\"?\")\n",
    "            img_url = img_url[:pos]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        # finally, if the url is valid\n",
    "        if is_valid(img_url):\n",
    "            urls.append(img_url)\n",
    "    return urls\n",
    "\n",
    "def download(url, pathname):\n",
    "    \"\"\"\n",
    "    Downloads a file given an URL and puts it in the folder `pathname`\n",
    "    \"\"\"\n",
    "    # if path doesn't exist, make that path dir\n",
    "    if not os.path.isdir(pathname):\n",
    "        os.makedirs(pathname)\n",
    "    # download the body of response by chunk, not immediately\n",
    "    response = requests.get(url, stream=True)\n",
    "    # get the total file size\n",
    "    file_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "    # get the file name\n",
    "    filename = os.path.join(pathname, url.split(\"/\")[-1])\n",
    "    # progress bar, changing the unit to bytes instead of iteration (default by tqdm)\n",
    "    progress = tqdm(response.iter_content(1024), f\"Downloading {filename}\", \\\n",
    "                    total=file_size, unit=\"B\", unit_scale=True, unit_divisor=1024)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        for data in progress:           \n",
    "            # write data read to the file\n",
    "            f.write(data)\n",
    "            # update the progress bar manually\n",
    "            progress.update(len(data))\n",
    "            \n",
    "def main(url, path):\n",
    "    # get all images\n",
    "    imgs = get_all_images(url)\n",
    "    for img in imgs:\n",
    "        # for each image, download it\n",
    "        download(img, path)\n",
    "        \n",
    "        \n",
    "main(\"https://www.thepythoncode.com/topic/web-scraping\", \"web-scraping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
